{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":33547,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":28079,"modelId":39106},{"sourceId":272868,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":233631,"modelId":255351},{"sourceId":272870,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":233632,"modelId":255352},{"sourceId":272886,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":233644,"modelId":255363}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. LLM训练","metadata":{}},{"cell_type":"markdown","source":"## 1.1 安装环境依赖","metadata":{}},{"cell_type":"code","source":"# !pip install /kaggle/input/bsd/other/default/1/bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl\n!pip install /kaggle/input/bsd44/other/default/1/bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:25.927479Z","iopub.execute_input":"2025-03-02T14:48:25.927800Z","iopub.status.idle":"2025-03-02T14:48:29.659717Z","shell.execute_reply.started":"2025-03-02T14:48:25.927767Z","shell.execute_reply":"2025-03-02T14:48:29.658709Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/bsd44/other/default/1/bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.44.0) (2.5.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.44.0) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.44.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.44.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.44.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.44.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.44.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bitsandbytes==0.44.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.44.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.44.0) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.44.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.44.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.44.0) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.44.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes==0.44.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.44.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes==0.44.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bitsandbytes==0.44.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bitsandbytes==0.44.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bitsandbytes==0.44.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bitsandbytes==0.44.0) (2024.2.0)\nbitsandbytes is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:29.660527Z","iopub.execute_input":"2025-03-02T14:48:29.660757Z","iopub.status.idle":"2025-03-02T14:48:29.998386Z","shell.execute_reply.started":"2025-03-02T14:48:29.660737Z","shell.execute_reply":"2025-03-02T14:48:29.997304Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama3_8b/pytorch/v1/1/llama_3_finetuned_model.pth\n/kaggle/input/llama-3/transformers/8b-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-hf/1/generation_config.json\n/kaggle/input/bsd44/other/default/1/bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl\n/kaggle/input/bsd/other/default/1/bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 1.2 导入相关依赖包","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:29.999645Z","iopub.execute_input":"2025-03-02T14:48:30.000079Z","iopub.status.idle":"2025-03-02T14:48:36.504764Z","shell.execute_reply.started":"2025-03-02T14:48:30.000035Z","shell.execute_reply":"2025-03-02T14:48:36.503677Z"}},"outputs":[{"name":"stdout","text":"Torch Version: 2.5.1+cu121\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 1.3 配置训练参数","metadata":{}},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 8\n    DROPOUT = 0.05\n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-hf/1'\n    SEED = 2024\n    MAX_LENGTH = 1024\n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5\n    NUM_LABELS = 3  # 分类的类别数量\n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:36.505551Z","iopub.execute_input":"2025-03-02T14:48:36.506058Z","iopub.status.idle":"2025-03-02T14:48:36.510402Z","shell.execute_reply.started":"2025-03-02T14:48:36.506034Z","shell.execute_reply":"2025-03-02T14:48:36.509420Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 1.4 固定随机种子","metadata":{}},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility\"\"\"\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n \nset_seeds(seed=CFG.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:36.513284Z","iopub.execute_input":"2025-03-02T14:48:36.513512Z","iopub.status.idle":"2025-03-02T14:48:36.557681Z","shell.execute_reply.started":"2025-03-02T14:48:36.513477Z","shell.execute_reply":"2025-03-02T14:48:36.557076Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 1.5 加载分词器","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:36.559124Z","iopub.execute_input":"2025-03-02T14:48:36.559315Z","iopub.status.idle":"2025-03-02T14:48:37.311432Z","shell.execute_reply.started":"2025-03-02T14:48:36.559297Z","shell.execute_reply":"2025-03-02T14:48:37.310465Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## 1.6 数据加载","metadata":{}},{"cell_type":"code","source":"def get_token_lengths(texts):\n    # tokenize and receive input_ids for each text\n    input_ids = tokenizer(texts.tolist(), return_tensors=\"pt\")[\"input_ids\"]\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]\n\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:37.312618Z","iopub.execute_input":"2025-03-02T14:48:37.313026Z","iopub.status.idle":"2025-03-02T14:48:39.056619Z","shell.execute_reply.started":"2025-03-02T14:48:37.312985Z","shell.execute_reply":"2025-03-02T14:48:39.055795Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def process(input_str):\n    stripped_str = input_str.strip('[]')    # 去除左右两侧的[]\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]   # 以\",\"分割，去除分割后字符串前后的\"\n    return ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n# Drop \"Null\" for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\") \nprint('Total train samples: ', len(train))\n\ntrain.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:39.057602Z","iopub.execute_input":"2025-03-02T14:48:39.057868Z","iopub.status.idle":"2025-03-02T14:48:39.471536Z","shell.execute_reply.started":"2025-03-02T14:48:39.057845Z","shell.execute_reply":"2025-03-02T14:48:39.470597Z"}},"outputs":[{"name":"stdout","text":"Total 19 Null response rows dropped\nTotal train samples:  57458\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  Is it morally right to try to have a certain p...   \n1  What is the difference between marriage licens...   \n2  explain function calling. how would you call a...   \n3  How can I create a test set for a very rare ca...   \n4  What is the best way to travel from Tel-Aviv t...   \n\n                                          response_a  \\\n0  The question of whether it is morally right to...   \n1  A marriage license is a legal document that al...   \n2  Function calling is the process of invoking or...   \n3  Creating a test set for a very rare category c...   \n4  The best way to travel from Tel Aviv to Jerusa...   \n\n                                          response_b  winner_model_a  \\\n0  As an AI, I don't have personal beliefs or opi...               1   \n1  A marriage license and a marriage certificate ...               0   \n2  Function calling is the process of invoking a ...               0   \n3  When building a classifier for a very rare cat...               1   \n4  The best way to travel from Tel-Aviv to Jerusa...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>Is it morally right to try to have a certain p...</td>\n      <td>The question of whether it is morally right to...</td>\n      <td>As an AI, I don't have personal beliefs or opi...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>What is the difference between marriage licens...</td>\n      <td>A marriage license is a legal document that al...</td>\n      <td>A marriage license and a marriage certificate ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>explain function calling. how would you call a...</td>\n      <td>Function calling is the process of invoking or...</td>\n      <td>Function calling is the process of invoking a ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>How can I create a test set for a very rare ca...</td>\n      <td>Creating a test set for a very rare category c...</td>\n      <td>When building a classifier for a very rare cat...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>What is the best way to travel from Tel-Aviv t...</td>\n      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 1.7 数据预处理","metadata":{}},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] + '\\n\\nModel A :\\n' + \\\ntrain['response_a'] + '\\n\\n--------\\n\\nModel B:\\n' + train['response_b']\nprint(type(train['text'][4]))\nprint(train['text'][4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:39.472573Z","iopub.execute_input":"2025-03-02T14:48:39.472943Z","iopub.status.idle":"2025-03-02T14:48:39.767886Z","shell.execute_reply.started":"2025-03-02T14:48:39.472909Z","shell.execute_reply":"2025-03-02T14:48:39.766955Z"}},"outputs":[{"name":"stdout","text":"<class 'str'>\nUser prompt: What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\n\nModel A :\nThe best way to travel from Tel Aviv to Jerusalem depends on your personal preference and the availability of transportation options. All the options you have mentioned are valid options, but here are some details to help you make your decision:\\n\\n*   By car: Traveling by car is the quickest way to get from Tel Aviv to Jerusalem, as the distance between the two cities is only about 60 kilometers (37 miles). It takes around 45 minutes to drive from Tel Aviv to Jerusalem by car, depending on the traffic.\\n*   By bus: There are several bus lines that run from Tel Aviv to Jerusalem, and the journey takes around 1 hour and 30 minutes by bus. The buses are comfortable and reliable, and they offer a scenic view of the beautiful Israeli countryside.\\n*   By plane: There are no direct flights from Tel Aviv to Jerusalem, so you need to take a flight from Tel Aviv's Ben Gurion International Airport to Jerusalem's Sde Dov Airport. The flight takes around 30 minutes, and it is a convenient option if you are short on time.\\n\\nIn general, if you have enough time and don't mind taking the bus, I recommend that option. The buses are comfortable, reliable, and offer a scenic view of the countryside. If you are short on time and don't want to take a bus, a flight is another option to consider.\n\n--------\n\nModel B:\nThe best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer to use public transportation, there are several bus lines that operate between Tel-Aviv and Jerusalem. Some of the most popular bus companies include Egged and Dan. The bus ride typically takes about an hour, depending on traffic.\\n\\nTaking a plane is not a recommended option since Tel-Aviv and Jerusalem are relatively close cities, and there are no airports in Jerusalem. \\n\\nIn summary, taking a bus is the most commonly used and convenient way to travel from Tel-Aviv to Jerusalem.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 1.8 模型训练","metadata":{}},{"cell_type":"markdown","source":"# 2. 推理和预测","metadata":{}},{"cell_type":"markdown","source":"## 2.1 导入依赖包","metadata":{}},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport time\n\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:39.768795Z","iopub.execute_input":"2025-03-02T14:48:39.769043Z","iopub.status.idle":"2025-03-02T14:48:39.774010Z","shell.execute_reply.started":"2025-03-02T14:48:39.769020Z","shell.execute_reply":"2025-03-02T14:48:39.773081Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 4.2 设置全局参数","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-hf/1'\nWEIGHTS_PATH = '/kaggle/input/llama3_8b/pytorch/v1/1/llama_3_finetuned_model.pth'\nMAX_LENGTH = 1024\nBATCH_SIZE = 8\nDEVICE = torch.device(\"cuda\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:39.774952Z","iopub.execute_input":"2025-03-02T14:48:39.775151Z","iopub.status.idle":"2025-03-02T14:48:39.792131Z","shell.execute_reply.started":"2025-03-02T14:48:39.775132Z","shell.execute_reply":"2025-03-02T14:48:39.791365Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 4.3 准备待预测数据","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n\n# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:50:48.423142Z","iopub.execute_input":"2025-03-02T14:50:48.423484Z","iopub.status.idle":"2025-03-02T14:50:48.445059Z","shell.execute_reply.started":"2025-03-02T14:50:48.423460Z","shell.execute_reply":"2025-03-02T14:50:48.444125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.333333        0.333333    0.333333\n1   211333        0.333333        0.333333    0.333333\n2  1233961        0.333333        0.333333    0.333333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        id                                             prompt  \\\n0   136060  I have three oranges today, I ate an orange ye...   \n1   211333  You are a mediator in a heated political debat...   \n2  1233961  How to initialize the classification head when...   \n\n                                          response_a  \\\n0                        You have two oranges today.   \n1  Thank you for sharing the details of the situa...   \n2  When you want to initialize the classification...   \n\n                                          response_b  \n0  You still have three oranges. Eating an orange...  \n1  Mr Reddy and Ms Blue both have valid points in...  \n2  To initialize the classification head when per...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>I have three oranges today, I ate an orange ye...</td>\n      <td>You have two oranges today.</td>\n      <td>You still have three oranges. Eating an orange...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>You are a mediator in a heated political debat...</td>\n      <td>Thank you for sharing the details of the situa...</td>\n      <td>Mr Reddy and Ms Blue both have valid points in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>How to initialize the classification head when...</td>\n      <td>When you want to initialize the classification...</td>\n      <td>To initialize the classification head when per...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:39.826418Z","iopub.execute_input":"2025-03-02T14:48:39.826693Z","iopub.status.idle":"2025-03-02T14:48:39.832068Z","shell.execute_reply.started":"2025-03-02T14:48:39.826656Z","shell.execute_reply":"2025-03-02T14:48:39.831174Z"}},"outputs":[{"name":"stdout","text":"User prompt: I have three oranges today, I ate an orange yesterday. How many oranges do I have?\n\nModel A :\nYou have two oranges today.\n\n--------\n\nModel B:\nYou still have three oranges. Eating an orange yesterday does not affect the number of oranges you have today.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 4.4 分词","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('./tokenizer')\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:39.832812Z","iopub.execute_input":"2025-03-02T14:48:39.833003Z","iopub.status.idle":"2025-03-02T14:48:40.731928Z","shell.execute_reply.started":"2025-03-02T14:48:39.832979Z","shell.execute_reply":"2025-03-02T14:48:40.730981Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                           INPUT_IDS  \\\n0  [1502, 10137, 25, 358, 617, 2380, 85138, 3432,...   \n1  [1502, 10137, 25, 1472, 527, 264, 69030, 304, ...   \n\n                                     ATTENTION_MASKS  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>INPUT_IDS</th>\n      <th>ATTENTION_MASKS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1502, 10137, 25, 358, 617, 2380, 85138, 3432,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1502, 10137, 25, 1472, 527, 264, 69030, 304, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## 4.5 加载模型","metadata":{}},{"cell_type":"code","source":"# BitsAndBytes configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n    bnb_8bit_use_double_quant=False)\n\n# 使用单个GPU加载基础模型\ndevice = torch.device('cuda')  # 指定使用的GPU设备\n\n# 加载基础模型\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map=device)  # 指定模型加载到的设备\nbase_model.config.pad_token_id = tokenizer.pad_token_id  # 配置模型的填充标记ID","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:40.732800Z","iopub.execute_input":"2025-03-02T14:48:40.733034Z","iopub.status.idle":"2025-03-02T14:48:56.778960Z","shell.execute_reply.started":"2025-03-02T14:48:40.733013Z","shell.execute_reply":"2025-03-02T14:48:56.778091Z"}},"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b87c99a66dd4279a787ee6e959b3747"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3/transformers/8b-hf/1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 4.6 加载LoRA模型参数","metadata":{}},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])\n\n# 获取peft模型并加载权重\nmodel = get_peft_model(base_model, peft_config).to(device)\nmodel.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel.eval()\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:56.779917Z","iopub.execute_input":"2025-03-02T14:48:56.780242Z","iopub.status.idle":"2025-03-02T14:48:56.930935Z","shell.execute_reply.started":"2025-03-02T14:48:56.780206Z","shell.execute_reply":"2025-03-02T14:48:56.930149Z"}},"outputs":[{"name":"stdout","text":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): LlamaForSequenceClassification(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n              (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear8bitLt(\n                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=4096, out_features=3, bias=False)\n        )\n      )\n    )\n  )\n)\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-16-bb02988f02f7>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 4.7 执行推理","metadata":{}},{"cell_type":"code","source":"def inference(df, model, device, batch_size=BATCH_SIZE):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df\n\n\n# 记录开始时间\nst = time.time()\n\n# 对整个数据集进行推理\ndata = inference(data, model, device)\n\n# 打印总耗时\nprint(f\"Processing complete. Total time: {time.time() - st}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:48:56.931963Z","iopub.execute_input":"2025-03-02T14:48:56.932300Z","iopub.status.idle":"2025-03-02T14:49:04.210156Z","shell.execute_reply.started":"2025-03-02T14:48:56.932266Z","shell.execute_reply":"2025-03-02T14:49:04.209093Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-a68a18f4edbc>:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Processing complete. Total time: 7.270376682281494\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:49:16.368518Z","iopub.execute_input":"2025-03-02T14:49:16.368832Z","iopub.status.idle":"2025-03-02T14:49:16.383544Z","shell.execute_reply.started":"2025-03-02T14:49:16.368809Z","shell.execute_reply":"2025-03-02T14:49:16.382819Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                           INPUT_IDS  \\\n0  [1502, 10137, 25, 358, 617, 2380, 85138, 3432,...   \n1  [1502, 10137, 25, 1472, 527, 264, 69030, 304, ...   \n2  [1502, 10137, 25, 2650, 311, 9656, 279, 24790,...   \n\n                                     ATTENTION_MASKS  winner_model_a  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        0.193481   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        0.465332   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        0.333740   \n\n   winner_model_b  winner_tie  \n0        0.272949    0.533691  \n1        0.303223    0.231323  \n2        0.384521    0.281738  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>INPUT_IDS</th>\n      <th>ATTENTION_MASKS</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1502, 10137, 25, 358, 617, 2380, 85138, 3432,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>0.193481</td>\n      <td>0.272949</td>\n      <td>0.533691</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1502, 10137, 25, 1472, 527, 264, 69030, 304, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>0.465332</td>\n      <td>0.303223</td>\n      <td>0.231323</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1502, 10137, 25, 2650, 311, 9656, 279, 24790,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>0.333740</td>\n      <td>0.384521</td>\n      <td>0.281738</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]\ndisplay(sample_sub)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:50:58.618099Z","iopub.execute_input":"2025-03-02T14:50:58.618413Z","iopub.status.idle":"2025-03-02T14:50:58.628806Z","shell.execute_reply.started":"2025-03-02T14:50:58.618388Z","shell.execute_reply":"2025-03-02T14:50:58.627807Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.193481        0.272949    0.533691\n1   211333        0.465332        0.303223    0.231323\n2  1233961        0.333740        0.384521    0.281738","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>136060</td>\n      <td>0.193481</td>\n      <td>0.272949</td>\n      <td>0.533691</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>211333</td>\n      <td>0.465332</td>\n      <td>0.303223</td>\n      <td>0.231323</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1233961</td>\n      <td>0.333740</td>\n      <td>0.384521</td>\n      <td>0.281738</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"sample_sub.to_csv('submission.csv', header=True, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:51:01.089787Z","iopub.execute_input":"2025-03-02T14:51:01.090092Z","iopub.status.idle":"2025-03-02T14:51:01.095382Z","shell.execute_reply.started":"2025-03-02T14:51:01.090069Z","shell.execute_reply":"2025-03-02T14:51:01.094314Z"}},"outputs":[],"execution_count":23}]}